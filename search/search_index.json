{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Me","text":"<p>I'm an Applied AI consultant with a strong interest in startups, venture capital and software engineering.</p> <p>My personal areas of interest in Applied AI are LLM evaluations.</p> <p>Quote</p> <p>The ideal engineer is a composite. He is not a scientist, he is not a mathematician, he is not a sociologist or a writer, but he may use the knowledge and techniques of any or all of these disciplines in solving engineering problems.</p>"},{"location":"#past-clients","title":"Past Clients","text":"<p>Some of my past clients include:</p> <ul> <li>Dell</li> <li>Razorpay</li> <li>Calsoft</li> <li>Athina AI</li> <li>Literal AI/Chainlit</li> <li>Giant Ventures</li> <li>Replit</li> <li>Arcee AI</li> <li>Arize AI</li> </ul> <p>This is a non-exhaustive list.</p>"},{"location":"#working-together","title":"Working Together","text":"<p>I usually operate in two ways:</p> <ol> <li>Secondment to your teams: act as a part of your team to help you solve your AI problems.</li> <li>IC: share me the context of your problem, and I build and fix it for you.</li> </ol> <p>Open to exploring other ways of working if it suits you and your teams.</p> <p> Block time to chat</p>"},{"location":"#experience","title":"Experience","text":""},{"location":"#engineering","title":"Engineering","text":"<ul> <li>Data Engineering at General Catalyst. Pushed GPT-3 in prod as early as 2022.</li> <li>Quant Developer at an Options trading fund, AUM 300K USD.</li> <li>SRE at Last9 Inc.</li> <li>Software Engineering at Sequoia Capital India.</li> <li>Founded Transfrm.ai, a GenAI consulting firm</li> <li>Google Summer of Code 2021 with OpenAstronomy.</li> <li>Google Season of Docs 2021 with HPX.</li> <li>Google Season of Docs 2022 with The R Foundation for Statistical Computing.</li> </ul>"},{"location":"#investing","title":"Investing","text":"<ul> <li>Associate with gradCapital.</li> <li>Investment analyst with EMVC and Speciale Invest.</li> <li>Venture Capital Fellow at Blume Ventures.</li> </ul>"},{"location":"#current-focus","title":"Current Focus","text":"<ul> <li>Exploring making LLMs more robust using evals and monitoring.</li> <li>Contributing to OSS (for fun!) </li> </ul>"},{"location":"contact/","title":"Contact","text":"<p>Get in touch with me for collaborations, consulting, or just to chat about AI and technology.</p>"},{"location":"contact/#connect","title":"Connect","text":"<ul> <li>GitHub: @rachittshah</li> <li>Twitter: @rachittshah</li> <li>LinkedIn: Rachitt Shah</li> <li>Email: contact@rachittshah.com</li> </ul>"},{"location":"contact/#work-with-me","title":"Work With Me","text":""},{"location":"contact/#consulting","title":"Consulting","text":"<p>I offer consulting services in: - RAG system design and implementation - LLM integration and optimization - Vector database selection and setup - AI system architecture</p>"},{"location":"contact/#speaking","title":"Speaking","text":"<p>Available for: - Technical workshops - Conference talks - Webinars - Panel discussions</p>"},{"location":"contact/#writing","title":"Writing","text":"<p>Open to: - Technical blog collaborations - Tutorial series - Documentation projects - Book contributions</p>"},{"location":"contact/#office-hours","title":"Office Hours","text":"<p>I hold regular office hours for the open-source community. Book a slot to discuss: - Technical challenges - Project architecture - Career guidance - Open source contributions</p> <p>Book Office Hours </p>"},{"location":"projects/","title":"Projects","text":"<p>A collection of my hobby projects exploring various aspects of AI, LLMs, and knowledge graphs.</p>"},{"location":"projects/#llm-evaluation-reasoning","title":"LLM Evaluation &amp; Reasoning","text":""},{"location":"projects/#reasoning-evaluations","title":"Reasoning Evaluations","text":"<p>A comprehensive framework for evaluating reasoning capabilities in Large Language Models. - GitHub Repository - Features: Systematic evaluation methods, reasoning benchmarks, performance analysis</p>"},{"location":"projects/#llamacpp-evaluations-with-dspy","title":"Llama.cpp Evaluations with DSPy","text":"<p>Custom evaluation framework for Llama models using DSPy. - GitHub Repository - Features: Model performance metrics, DSPy integration, efficient evaluation pipelines</p>"},{"location":"projects/#tree-of-thought-with-lmql","title":"Tree of Thought with LMQL","text":"<p>Implementation of Tree of Thought reasoning using LMQL for structured LLM interactions. - GitHub Repository - Features: Advanced reasoning patterns, LMQL integration, thought process visualization</p>"},{"location":"projects/#knowledge-graphs-recommendations","title":"Knowledge Graphs &amp; Recommendations","text":""},{"location":"projects/#linkedin-knowledge-graph-recommendations","title":"LinkedIn Knowledge Graph Recommendations","text":"<p>Knowledge graph-based recommendation system using LinkedIn data. - GitHub Repository - Features: Graph-based recommendations, professional network analysis</p>"},{"location":"projects/#dspy-knowledge-graphs","title":"DSPy Knowledge Graphs","text":"<p>Exploring knowledge graph construction and querying with DSPy. - GitHub Repository - Features: Knowledge graph generation, DSPy integration, semantic querying</p>"},{"location":"projects/#prompt-engineering-experimentation","title":"Prompt Engineering &amp; Experimentation","text":""},{"location":"projects/#dspy-prompt-builder","title":"DSPy Prompt Builder","text":"<p>Interactive tool for building and optimizing prompts using DSPy. - GitHub Repository - Features: Prompt templates, optimization strategies, DSPy integration</p>"},{"location":"projects/#pgit-github-for-prompts","title":"PGit - GitHub for Prompts","text":"<p>Version control and management system specifically designed for LLM prompts. - GitHub Repository - Features: Prompt versioning, collaboration tools, prompt history tracking</p>"},{"location":"projects/#prompt-evaluation-learnings","title":"Prompt Evaluation Learnings","text":"<p>Collection of insights and best practices for prompt engineering and evaluation. - GitHub Repository - Features: Best practices, evaluation metrics, case studies</p>"},{"location":"projects/#fine-tuning-experimentation","title":"Fine-tuning &amp; Experimentation","text":""},{"location":"projects/#fine-tuning-experiments","title":"Fine-tuning Experiments","text":"<p>Collection of experiments with various fine-tuning approaches for LLMs. - GitHub Repository - Features: Different fine-tuning methods, performance comparisons, optimization techniques</p>"},{"location":"projects/#langchain-projects","title":"LangChain Projects","text":""},{"location":"projects/#cov-langchain","title":"CoV LangChain","text":"<p>Chain of Verification implementation using LangChain. - GitHub Repository - Features: Verification chains, accuracy improvements, LangChain integration</p>"},{"location":"projects/#langchain-experiments","title":"LangChain Experiments","text":"<p>Various experimental implementations and use cases with LangChain. - GitHub Repository - Features: Different chain types, use case implementations, best practices </p>"},{"location":"ai/","title":"AI Engineering","text":"<p>Welcome to the AI Engineering section! Here you'll find comprehensive guides and tutorials on building AI-powered systems, with a focus on practical implementations and best practices.</p>"},{"location":"ai/#topics-covered","title":"Topics Covered","text":"<ul> <li>Vector Databases - Deep dive into vector storage and similarity search</li> <li>LLM Integration - Guides for integrating language models effectively</li> <li>System architecture and scalability</li> <li>Performance optimization</li> <li>Testing and monitoring</li> </ul>"},{"location":"ai/#getting-started","title":"Getting Started","text":"<p>If you're new to AI Engineering, start with the fundamentals in the Vector Databases section, then explore LLM integration patterns. </p>"},{"location":"ai/llm-integration/","title":"LLM Integration","text":"<p>A comprehensive guide to integrating Large Language Models (LLMs) into applications.</p>"},{"location":"ai/llm-integration/#introduction","title":"Introduction","text":"<p>Effective LLM integration requires careful consideration of architecture, performance, cost, and user experience. This guide covers key patterns and best practices.</p>"},{"location":"ai/llm-integration/#integration-patterns","title":"Integration Patterns","text":""},{"location":"ai/llm-integration/#direct-integration","title":"Direct Integration","text":"<ul> <li>API-based integration</li> <li>Self-hosted models</li> <li>Batch processing</li> <li>Streaming responses</li> </ul>"},{"location":"ai/llm-integration/#rag-integration","title":"RAG Integration","text":"<ul> <li>Document processing</li> <li>Vector storage</li> <li>Query processing</li> <li>Response generation</li> </ul>"},{"location":"ai/llm-integration/#fine-tuning","title":"Fine-tuning","text":"<ul> <li>Dataset preparation</li> <li>Training strategies</li> <li>Model evaluation</li> <li>Deployment considerations</li> </ul>"},{"location":"ai/llm-integration/#best-practices","title":"Best Practices","text":""},{"location":"ai/llm-integration/#prompt-engineering","title":"Prompt Engineering","text":"<ul> <li>Prompt design patterns</li> <li>System messages</li> <li>Few-shot learning</li> <li>Output formatting</li> </ul>"},{"location":"ai/llm-integration/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Caching strategies</li> <li>Batch processing</li> <li>Response streaming</li> <li>Load balancing</li> </ul>"},{"location":"ai/llm-integration/#cost-management","title":"Cost Management","text":"<ul> <li>Token optimization</li> <li>Model selection</li> <li>Caching policies</li> <li>Usage monitoring</li> </ul>"},{"location":"ai/llm-integration/#error-handling","title":"Error Handling","text":"<ul> <li>Rate limiting</li> <li>Fallback strategies</li> <li>Retry mechanisms</li> <li>Monitoring and alerts</li> </ul>"},{"location":"ai/llm-integration/#security-considerations","title":"Security Considerations","text":""},{"location":"ai/llm-integration/#data-privacy","title":"Data Privacy","text":"<ul> <li>Input sanitization</li> <li>Output filtering</li> <li>PII handling</li> <li>Audit logging</li> </ul>"},{"location":"ai/llm-integration/#model-security","title":"Model Security","text":"<ul> <li>Access control</li> <li>API key management</li> <li>Request validation</li> <li>Response filtering</li> </ul>"},{"location":"ai/llm-integration/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"ai/llm-integration/#key-metrics","title":"Key Metrics","text":"<ul> <li>Response times</li> <li>Token usage</li> <li>Error rates</li> <li>Cost per request</li> </ul>"},{"location":"ai/llm-integration/#quality-assurance","title":"Quality Assurance","text":"<ul> <li>Output validation</li> <li>Semantic accuracy</li> <li>Consistency checks</li> <li>User feedback </li> </ul>"},{"location":"ai/vector-databases/","title":"Vector Databases","text":"<p>A comprehensive guide to vector databases and similarity search in AI applications.</p>"},{"location":"ai/vector-databases/#introduction","title":"Introduction","text":"<p>Vector databases are specialized database systems designed to store and efficiently query high-dimensional vectors, which are essential for modern AI applications.</p>"},{"location":"ai/vector-databases/#key-concepts","title":"Key Concepts","text":""},{"location":"ai/vector-databases/#vector-embeddings","title":"Vector Embeddings","text":"<ul> <li>Understanding vector representations</li> <li>Embedding models and techniques</li> <li>Dimensionality considerations</li> <li>Quality and consistency</li> </ul>"},{"location":"ai/vector-databases/#similarity-search","title":"Similarity Search","text":"<ul> <li>Distance metrics</li> <li>Approximate Nearest Neighbors (ANN)</li> <li>Trade-offs in accuracy vs. speed</li> <li>Filtering and hybrid search</li> </ul>"},{"location":"ai/vector-databases/#indexing-methods","title":"Indexing Methods","text":"<ul> <li>HNSW</li> <li>IVF</li> <li>Product Quantization</li> <li>Tree-based methods</li> </ul>"},{"location":"ai/vector-databases/#popular-solutions","title":"Popular Solutions","text":""},{"location":"ai/vector-databases/#open-source","title":"Open Source","text":"<ul> <li>Milvus</li> <li>Weaviate</li> <li>Qdrant</li> <li>FAISS</li> </ul>"},{"location":"ai/vector-databases/#cloud-services","title":"Cloud Services","text":"<ul> <li>Pinecone</li> <li>Weaviate Cloud</li> <li>Azure Vector Search</li> <li>AWS OpenSearch</li> </ul>"},{"location":"ai/vector-databases/#best-practices","title":"Best Practices","text":""},{"location":"ai/vector-databases/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>Index tuning</li> <li>Batch processing</li> <li>Caching strategies</li> <li>Hardware considerations</li> </ul>"},{"location":"ai/vector-databases/#scalability","title":"Scalability","text":"<ul> <li>Sharding strategies</li> <li>Replication</li> <li>Load balancing</li> <li>Monitoring and metrics</li> </ul>"},{"location":"ai/vector-databases/#integration-patterns","title":"Integration Patterns","text":"<ul> <li>Bulk operations</li> <li>Real-time updates</li> <li>Error handling</li> <li>Backup and recovery </li> </ul>"},{"location":"blog/","title":"Blog Posts","text":"<p>Sharing my views, opinions and learnings on AI Engineering, RAG Systems, Venture Capital, and more.</p>"},{"location":"blog/#categories","title":"Categories","text":"<ul> <li>AI Engineering &amp; LLMs</li> <li>RAG Systems</li> <li>Venture Capital</li> <li>Technical Notes</li> <li>Career &amp; Personal Growth</li> </ul>"},{"location":"blog/#subscribe","title":"Subscribe","text":"<p>You can subscribe to my blog updates through: - RSS Feed - Twitter - LinkedIn </p>"},{"location":"blog/tags/","title":"Tags","text":"<p>Browse posts by topic:</p> <p>[TAGS] </p>"},{"location":"blog/2022/03/17/blog-post-title-from-file-name/","title":"Blog Post Title From File Name","text":"","tags":["blog","introduction"]},{"location":"blog/2022/03/17/blog-post-title-from-file-name/#hello-world","title":"Hello World","text":"<p>The main motive behind this blog is to share what I write and share who I am, in a minimalist way. Instead of going for a fancy framework, I choose GH pages+Jekyll for ease of maintaning it. </p> <p>Stay connected, stranger. </p>","tags":["blog","introduction"]},{"location":"blog/2022/03/18/venture-capital-memos/","title":"Venture Capital Memos","text":"","tags":["venture-capital","investing","memos"]},{"location":"blog/2022/03/18/venture-capital-memos/#venture-capital-memos","title":"Venture Capital Memos","text":"<p>A very common question asked by aspiring VCs is where do we learn how investments are made? VC is largely a learning on the job role. Feedback cycles take years at the minimum.</p> <p>How do you think like a VC?</p> <p>Enter investment memos: these small documents highlight the usual decision making process of a Venture Capital fund. Many at times, fundraises have memos as well.</p>","tags":["venture-capital","investing","memos"]},{"location":"blog/2022/03/18/venture-capital-memos/#list-of-memos-to-learn","title":"List of memos to learn","text":"<ul> <li>BVP memos</li> <li>Sino Global</li> <li>Memo Collection</li> <li>Angel Investment memos</li> <li>AWS Memo</li> </ul>","tags":["venture-capital","investing","memos"]},{"location":"blog/2022/03/18/venture-capital-memos/#memo-templates","title":"Memo templates","text":"<ul> <li>YC memo template</li> <li>Nextview Ventures</li> <li>Fort Capital</li> <li>Visible VC</li> </ul> <p>Note :I am in no way involved with any of these funds.  If you wish to learn more about VC, drop me a line at LinkedIn</p>","tags":["venture-capital","investing","memos"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/","title":"Venture Capital investments in Open Source","text":"","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#problem","title":"Problem :","text":"<p>Understanding FOSS technologies and investing in open-source tech.</p> <p>Commerical platforms put you at the mercy of closed-source companies. You are subject to\u00a0vendor lock-in\u00a0and\u00a0rent-seeking behavior.</p> <p>No way to combat prices, raise features and get community support.</p>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#solution","title":"Solution","text":"<p>Monetized open-source\u00a0projects give some or all of the code away for free.</p> <p>With the ability to change it.Projects are\u00a0monetized\u00a0via services, premium features, hosting and more. Contributors are users themselves.</p> <p>Freemium plans for user-acquisition, lower CAC, better retention</p>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#key-players","title":"Key Players","text":"<p>Projects</p> <ul> <li>WordPress\u00a0\u2022 The most popular website builder</li> <li>Plausible\u00a0\u2022 Google Analytics alternative</li> <li>Elastic\u00a0\u2022 Search, analyze and visualize data</li> <li>Medusa\u00a0\u2022 Shopify alternative</li> <li>Builder\u00a0\u2022 Visual website builder</li> <li>Posthog\u00a0\u2022 Alternative to Mixpanel and Amplitude</li> <li>Supabase\u00a0\u2022 Firebase alternative</li> <li>Semgrep\u00a0\u2022 Static analysis tool</li> <li>AnonAddy\u00a0\u2022 Anonymous email forwarding</li> <li>Penpot\u00a0\u2022 Figma alternative</li> <li>Canonical \u2022 Linux OS</li> </ul> <p>India:</p> <ul> <li>Appsmith: Raised from Accel India</li> <li>Tooljet: Raised from Nexus Venture Partners</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#predictions","title":"Predictions","text":"<ul> <li>Monetized open source projects will become more\u00a0ambitious.<ul> <li>NocoDB\u00a0and\u00a0Baserow\u00a0are\u00a0Airtable\u00a0alternatives.</li> <li>n8n\u00a0is an alternative to\u00a0Zapier\u00a0and\u00a0Integromat.</li> <li>Medusa\u00a0is a\u00a0Shopify\u00a0alternative.</li> <li>Supabase is a Firebase alternative.</li> <li>Appsmith is a Retool alternative.</li> </ul> </li> <li>Networks will become public goods. Centralized networks have been one of the most effective ways to generate wealth.</li> <li>The Global Open Source Services Market size is expected to reach $60 billion by 2027, rising at a market growth of 17% CAGR during the forecast period. The term 'open source' refers to a kind of licensing agreement, which enables users to independently alter a work, combine work with big projects, use a work in different ways or develop a new workout based on the authentic.</li> <li>The rising number of skilled developers is solving for India for the world. More developers have contributed to Open Source from India than any other country.</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#growth","title":"Growth","text":"<ul> <li>Self-service selling\u00a0dramatically reduces the cost of selling and servicing transactional, lower-revenue deals. The product becomes a vehicle for allowing customers to expand their spending through executing upsells, particularly in usage-based pricing models.\u00a0Chargebee\u00a0offers 2 self-service plans in addition to a free offering, which enables SMB and mid-market customers to grow with the platform without needing to talk to sales.</li> <li>Data-driven targeting\u00a0uses product data in sales targeting and upsells motion; for instance, providing your sales team with a list of customers who are above their usage limits and ready to pay.\u00a0Nearpod\u00a0collects data around which teachers are actively using its platform and then leverages this to encourage school districts to become customers.</li> <li>New or premium feature adoption\u00a0can be improved by guiding users to that feature with product popups and callouts based on their usage patterns and use-case.\u00a0MURAL\u00a0surfaces new features in-app with contextual triggers, while also providing an in-depth changelog to demonstrate all of the value they are continuously adding to their product.</li> <li>Community development\u00a0involves fostering a community of users who can help each other understand the product and develop new innovations in usage. This community will form a key source of product advocacy and evangelism, with paying customers advocating to free users, and free users evangelizing the product to prospective users.\u00a0You should support and foster the community by providing forums and events (user conferences and smaller gatherings) where they can interact, uplifting the most active users of the community, and ensuring company employees interact and become members of the community themselves.\u00a0Postman\u00a0has fostered an active community of developers using the product and continues to invest in it.</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#opportunities","title":"Opportunities","text":"<ul> <li>The counter position\u00a0to\u00a0compete\u00a0with incumbents. Make it hard for them to mimic your strategy. Copying should lead to cannibalizing their existing business.\u00a0Medusa\u00a0is a\u00a0Shopify\u00a0alternative. It's unlikely that Shopify will open source. Even if Medusa becomes a formidable competitor.</li> <li>Permissionless contribute\u00a0to open-source projects. Use open-source contributions to build a portfolio and find jobs. Laszlo Block, former VP of People Operations at Google,\u00a0says\u00a0the number of Google employees without a college education is rising.</li> <li>Open-source alternatives avoid\u00a0platform risk,\u00a0vendor lock-in\u00a0and\u00a0rent-seeking behavior. Use code that you can\u00a0fork and self-host. Platforms\u00a0can raise prices without providing more value because switching costs are high.</li> <li>Turn complaints into\u00a0contributions. Accept\u00a0pull requests. Along with feature requests. Augment your dev team with open-source contributions.</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#security","title":"Security","text":"<ul> <li>securing serverless in the public cloud, perhaps by isolating serverless workloads in the public cloud with granular account-level segmentation, and limiting exposure through the use of blast-radius architecture</li> <li>rethinking authentication for transient serverless workloads by using ephemeral credentials and short-lived tokens, which are key risk mitigators for credential exposure</li> <li>protecting your availability in a serverless landscape with robust perimeter security that deploys public and internal functions at discrete gateways</li> <li>upgrading risk assessment, governance, and awareness by, for example, adopting policy as code for the codification of organizational policies; using regulatory frameworks in automated governance pipelines for cloud-service provisioning; and deploying all serverless workloads using an embedded\u00a0DevSecOps\u00a0pipeline</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#risks","title":"Risks","text":"<ul> <li>Consulting: Services\u00a0are less scalable than hosting and dual-license models. Consider marginal costs before you choose this strategy.</li> <li>Community Backlash\u00a0\u2022 Moving free features to paid tiers may lead to a backlash.</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#key-lessons","title":"Key Lessons","text":"<ul> <li>Users\u00a0are the real winners of monetized open source. Public goods have less lock-in.</li> <li>Value creation\u00a0does not automatically lead to value capture. Two-way rating systems, proof of stake protocols, and airdrops are uncornered innovations.</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#issues","title":"Issues","text":"<ul> <li>IP\u00a0rights will be hard to enforce in the new world.\u00a0CryptoPunks creators\u00a0are under fire for an inconsistent approach to derivatives.\u00a0Bored Apes\u00a0are slightly more lenient.\u00a0NFTs\u00a0gave us digital scarcity.\u00a0Legitimacy\u00a0and social consensus will rule the day.</li> <li>Wealthy\u00a0benefactors\u00a0will use open-source projects to prop up closed ecosystems and attract talent. See\u00a0Apple with Swift\u00a0and\u00a0Meta with React.</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/18/venture-capital-in-open-source/#links","title":"Links","text":"<ul> <li>https://www.insightpartners.com/blog/product-led-growth-the-new-paradigm-in-software-selling/#:~:text=Open source is offering source,a freemium or free trial.</li> <li>https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/saas-open-source-and-serverless-a-winning-combination-to-build-and-scale-new-businesses</li> <li>https://www.mckinsey.com/business-functions/mckinsey-digital/our-insights/saas-open-source-and-serverless-a-winning-combination-to-build-and-scale-new-businesses</li> <li>https://msupernaut.com/2021/11/12/open-source-%f0%9f%92%9c-venture-capital/</li> </ul>","tags":["venture-capital","open-source","investing"]},{"location":"blog/2022/03/19/how-to-cold-email/","title":"How to Cold Email","text":"","tags":["career","networking","communication"]},{"location":"blog/2022/03/19/how-to-cold-email/#how-to-write-a-good-cold-email","title":"How to write a good cold email?","text":"<p>If you're looking for job,internship, customers or fundraising, a cold email is your best shoot at reaching out(unless you know people from the organization).</p> <p>The beauty of cold emailing is that it's a direct and upfront ask, you know what you want, if you don't define and send that email.</p> <ul> <li>One line about who you are, and what do you do.</li> <li>Hello {receiver}, I'm {name}, studying/working with {name} and I wish to {request}</li> <li>Sharing about yourself</li> <li>Adding 2 to 5 lines about the key highlights which make you a good fit for the request you've asked.</li> <li>Be direct, short and crisp. Writing essays isn't saving anyone's time. Be concise and respectful.</li> <li>Quantify and share your proof of work. If you worked on a feature, show the time it saved or costs reduced. If you gained 100 customers, add that. Show numbers so people understand you better.</li> <li>Add all relevant links in the email. The reader isn't gonna spend his time looking you up. Respect their time.</li> <li>One line about who you are, and what do you do.</li> <li>Close the email wiht a call to action. Add a personal touch, get as personalized as possible!</li> </ul> <p>Sounds interesting? Write a cold email and get what you want! Drop me text at LinkedIn if it helps.</p>","tags":["career","networking","communication"]},{"location":"blog/2022/06/28/vc-modelling/","title":"VC Modelling","text":"","tags":["venture-capital","finance","modeling"]},{"location":"blog/2022/06/28/vc-modelling/#venture-capital-financial-modelling-notebook","title":"Venture Capital financial modelling notebook","text":"<p>I've always wanted to cover how captables are modelled. Using this github repo, I show how can we build captables, calculate expenses and EBIT, and company valuations.</p> <p>Link to github repo.</p>","tags":["venture-capital","finance","modeling"]},{"location":"blog/2022/06/28/vc-modelling/#free-cash-flow","title":"Free Cash Flow:","text":"<p>Free cash flow (FCF) represents the cash a company generates after accounting for cash outflows to support operations and maintain its capital assets.</p> <p>There are two main approaches to calculating FCF. The first approach uses cash flow from operating activities as the starting point, and then makes adjustments for interest expense, the tax shield on interest expense, and any capital expenditures (CapEx) undertaken that year. </p> <p>The second approach uses earnings before interest and taxes (EBIT) as the starting point, then adjusts for income taxes, non-cash expenses such as depreciation and amortization, changes in working capital, and CapEx. In both cases, the resulting numbers should be identical, but one approach may be preferred over the other depending on what financial information is available.</p>","tags":["venture-capital","finance","modeling"]},{"location":"blog/2022/06/28/vc-modelling/#ebit","title":"EBIT","text":"<p>Earnings before interest and taxes (EBIT) is an indicator of a company's profitability. EBIT can be calculated as revenue minus expenses excluding tax and interest. EBIT is also referred to as operating earnings, operating profit, and profit before interest and taxes.</p>","tags":["venture-capital","finance","modeling"]},{"location":"blog/2022/06/28/vc-modelling/#valuation","title":"Valuation","text":"<p>A business valuation is the process of determining the economic value of a business, giving owners an objective estimate of the value of their company. Typically, a business valuation happens when an owner is looking to sell all or a part of their business, or merge with another company.</p>","tags":["venture-capital","finance","modeling"]},{"location":"blog/2022/06/28/vc-modelling/#prepost-valuations","title":"Pre/Post Valuations","text":"<p>Pre-money and post-money differ in the timing of valuation. Pre-money valuation refers to the value of a company not including external funding or the latest round of funding. Post-money valuation includes outside financing or the latest capital injection.</p>","tags":["venture-capital","finance","modeling"]},{"location":"blog/2022/06/28/vc-modelling/#captable","title":"Captable","text":"<p>A capitalization table is a table providing an analysis of a company's percentages of ownership, equity dilution, and value of equity in each round of investment by founders, investors, and other owners.</p>","tags":["venture-capital","finance","modeling"]},{"location":"blog/2022/06/28/vc-modelling/#todo","title":"Todo","text":"<ul> <li> Add how to model captables</li> <li> How EBIT is measured</li> <li> Formulas for measurements</li> <li> Add info about termsheets</li> </ul>","tags":["venture-capital","finance","modeling"]},{"location":"blog/2023/12/17/llmops-101-logging-and-monitoring-llms/","title":"LLMOps 101","text":"<p>When we integrate LLMs into our applications, there's a major boost in the features of your apps. However, as with any complex system, it is crucial to have mechanisms in place that allow us to monitor and evaluate their performance over time. </p> <p>Logging is a fundamental aspect of this oversight. This blog post will explore why logging is essential for those utilizing LLMs any products, and how to follow an evaluation framework and production monitoring approach.</p> <p>Building apps with LLMs has the same principles as software engineering, with a focus on building reliable and scalable apps. </p>","tags":["llm","mlops","monitoring","production"]},{"location":"blog/2023/12/17/llmops-101-logging-and-monitoring-llms/#understanding-llm-behavior","title":"Understanding LLM Behavior","text":"<p>LLMs are inherently non-deterministic; the same input can lead to different outputs depending on various factors, including the model's training and the context provided. Logging helps us to record each interaction with the LLM, enabling us to understand its behavior by analyzing its responses over time. This is the foundation of building a reliable AI-powered application.</p> <p>Some key KPIs to observe:</p> <ul> <li>Cost Analysis: By tracking the cost of requests in real time, developers can manage budgets effectively and forecast expenses with greater accuracy.</li> <li>Token Metrics: Understanding token usage helps optimize prompt design, potentially lowering costs and improving response quality.</li> <li>Latency Averages: Performance is key in user experience. Mean latency metrics are crucial for identifying lags and making necessary optimizations.</li> <li>Success and Failure Rates: Real-time assessment of request outcomes enables developers to swiftly address failures and enhance success rates.</li> <li>User Engagement: Identifying your user base and their usage patterns allows for better targeting and personalization strategies.</li> <li>Model Popularity: Knowing which models are most used can guide decisions about future integrations or depreciations.</li> </ul>","tags":["llm","mlops","monitoring","production"]},{"location":"blog/2023/12/17/llmops-101-logging-and-monitoring-llms/#identifying-and-resolving-errors","title":"Identifying and Resolving Errors","text":"<p>Even the most advanced LLMs can produce errors, including misunderstandings, non sequiturs, and \"hallucinations,\" where the model confidently presents incorrect information. Logging is essential for identifying when and why these errors occur. By analyzing the logs, we can tweak our prompts or the model's parameters to reduce the incidence of such errors and improve the overall accuracy of the system.</p> <ul> <li>Error Rate Diagnosis: A high-level view of errors can pinpoint systemic issues that need immediate attention.</li> <li>Error Type Distribution: Classifying errors helps in understanding what kind of issues are most prevalent and how to prioritize fixes.</li> <li>Error Trend Analysis: Observing the trends of errors over time can indicate the robustness of newly released features or models.</li> <li>Rescue Success: Automatic retries and fallbacks are a safety net; monitoring their success helps ensure reliability even in failure scenarios.</li> </ul>","tags":["llm","mlops","monitoring","production"]},{"location":"blog/2023/12/17/llmops-101-logging-and-monitoring-llms/#measuring-performance-and-reliability","title":"Measuring Performance and Reliability","text":"<p>In order to ensure that an LLM is performing optimally and reliably within an application, it's vital to track specific KPIs that can provide actionable insights into its behavior. These KPIs help in understanding how the LLM interacts with users and handles various queries, thus informing decisions on system improvements and optimizations.</p> <p>Some key KPIs for measuring performance and reliability include:</p> <ul> <li>Response Time: The average time taken for the LLM to respond to a query. This KPI is crucial for user satisfaction as it directly impacts the user experience.</li> <li>Uptime / Availability: The percentage of time the LLM is operational and available for use without any outages, indicating system reliability.</li> <li>Error Rate: The ratio of the number of failed requests to the total number of requests, which helps in pinpointing stability issues.</li> <li>Success Rate: The percentage of queries handled successfully without any errors or interventions, showcasing the efficacy of the LLM.</li> <li>Recovery Time: The average time it takes for the LLM to recover from an error or failure, reflecting the resilience of the system.</li> <li>Quality of Responses: Measure the accuracy and relevance of the LLM's responses through qualitative analysis or user ratings.</li> <li>Throughput: The number of requests processed by the LLM in a given time frame, indicating the system's capacity to handle load.</li> <li>Fallback Rate: The frequency with which the system needs to resort to fallback mechanisms due to LLM's inability to provide an appropriate response.</li> <li>Repeat Interaction Rate: The rate at which users need to ask follow-up questions to get satisfactory answers, shedding light on the clarity and completeness of the LLM's responses.</li> <li>Benchmark Against Goals: How the LLM's performance aligns with predefined benchmarks or objectives for various metrics, reflecting whether the system meets the set performance goals.</li> </ul>","tags":["llm","mlops","monitoring","production"]},{"location":"blog/2023/12/17/llmops-101-logging-and-monitoring-llms/#feedback-loops-for-machine-learning","title":"Feedback Loops for Machine Learning","text":"<p>For LLMs to improve, they need high-quality, structured data to learn from. Logging provides invaluable data about the model's inputs and outputs, which can be used for further training and refinement. This creates a feedback loop where the performance of the LLM is continuously improved based on actual usage data.</p> <ul> <li>Feedback Volume: Keeping a pulse on how much feedback you're receiving is essential for understanding user engagement.</li> <li>Feedback Quality: Inspecting the scores and sentiments in feedback helps gauge user happiness and areas needing improvement.</li> <li>Trend Analysis: A trend line of feedback over time allows developers to measure the impact of changes and maintain a trajectory of improvement.</li> <li>Engaged User Base: Knowing who is providing feedback can help develop a community of testers and brand advocates.</li> </ul>","tags":["llm","mlops","monitoring","production"]},{"location":"blog/2023/12/17/llmops-101-logging-and-monitoring-llms/#enhancing-user-experience","title":"Enhancing User Experience","text":"<p>User feedback is a vital aspect of improving AI applications. By combining user feedback with detailed logs of LLM interactions, we can understand how users perceive the LLM's responses and identify areas where the user experience can be enhanced.</p> <ul> <li>Unique Users: Quantifying individual users helps in understanding the reach of your app and catering to diverse needs.</li> <li>Top Users: Identifying power users can help in community building and finding champions for your product.</li> <li>Usage Frequency: Tracking how often users engage with your app sheds light on its stickiness and daily relevance.</li> <li>Feedback Interaction: User feedback serves as a direct line to customer satisfaction and is critical for iterative development.</li> </ul>","tags":["llm","mlops","monitoring","production"]},{"location":"blog/2023/12/17/llmops-101-logging-and-monitoring-llms/#business-and-operational-insights","title":"Business and Operational Insights","text":"<p>Logs can reveal trends in how users interact with the application, providing business insights such as the peak times for LLM usage, the most common types of queries, or areas where users frequently encounter difficulties. This information is essential for operational planning and for developing strategies to encourage more effective use of the AI system.</p> <ul> <li>Unique Users: Quantifying individual users helps in understanding the reach of your app and catering to diverse needs.</li> <li>Top Users: Identifying power users can help in community building and finding champions for your product.</li> <li>Usage Frequency: Tracking how often users engage with your app sheds light on its stickiness and daily relevance.</li> <li>Feedback Interaction: User feedback serves as a direct line to customer satisfaction and is critical for iterative development.</li> </ul>","tags":["llm","mlops","monitoring","production"]},{"location":"blog/2024/01/01/evaluation-driven-development/","title":"Evaluation Driven Development","text":"<p>As a software engineer, I've been a huge fun of using test driven development(TDD), but how do we test something like LLMs? LLMs are far away from classical ML models, a stark difference is traditional ML models are often good at doing one task, but LLMs can do a variety of them.</p> <p>Thinking about it as a engineer, it used to scare me when each API returns a new response, not a very reliable system, is it?</p> <p>That's why I believe everyone building with LLMs needs to adopt the practice of using evals while building your LLM powered applications. </p> <p>For any software application, backward compatibility is a vital aspect to consider. It ensures that newer versions of the software will work with older ones, maintaining functionality and preventing inconveniences that can arise when making upgrades. When it comes to LLMs, however, ensuring backward compatibility can be a substantial challenge.</p> <p>LLM software, by its very nature, is continually evolving, with newer and more advanced models being released regularly. While one might assume that these software improvements would have linear upward trajectory in terms of performance and usability, the truth is, it may not be the same for certain specific use cases. Upgrading from an earlier model to a newer one might cause discrepancies in results since each model has its unique characteristics and behaviours.</p> <p>Ensuring backward compatibility, while challenging, is not an impossible task. Below are some strategies that can help in achieving this:</p> <ul> <li>Prompts Standardization: Developing a standardized syntax or nomenclature for prompts can go a long way in ensuring backward compatibility. This would imply a set of universally recognized and accepted guidelines for writing prompts that apply to all existing models and also newer ones.</li> <li>Thorough Testing: A robust testing process can help mitigate compatibility issues. Older prompts should be rigorously tested with newer models. Any prompt that does not achieve the desired or expected result should be adjusted or rewritten.</li> <li>Establish Clear Documentation: Detailed documentation of all prompts is crucial. When a developer understands the original intentions and structures behind a prompt, they will be better equipped to make necessary adjustments with newer models.</li> <li>Creating Model-Specific Code Paths: For critical applications where backward compatibility must be maintained, developers could consider running two versions of the model (the older and the newer one) and switch between them based on the situation. The decision on which model to use could depend on the prompts or the quality of responses.</li> </ul>","tags":["llm","development","evaluation","best-practices"]},{"location":"blog/2024/01/01/evaluation-driven-development/#how-do-you-actually-use-evals","title":"How do you actually use evals?","text":"<p>Using evals largely differs on what you're building. </p> <p>The best starting point to take while building is think evals first, how will the user prompt the LLM and what are the odds of the LLM returning the correct text? </p> <p>There is also the issue of 'prompt compatibility'. A prompt that worked perfectly with an older model may not necessarily evoke the intended response from a newer one, compelling developers to re-engineer their prompts, which can be quite a time-consuming task.</p> <p>IMO, starting from evals and going into the application behaviour is a great way to replicate TDD while building applications! Some things to consider:</p> <ul> <li>User behaviour</li> <li>Type of application</li> <li>Complexity: are you building agents or chains?</li> <li>Tools provided to the base LLM</li> <li>Prompt engineering: setting roles is a good hack and will ensure better evals!</li> </ul>","tags":["llm","development","evaluation","best-practices"]},{"location":"blog/2024/01/01/evaluation-driven-development/#metrics-that-matter","title":"Metrics that matter","text":"<p>In GenAI, user stickiness seems to be the largest issue, if you're a devtool or a consumer app, retention is a pain. Evals can in part help eliminate this by ensuring completion quality. </p> <p>Some key KPIs:</p> <ul> <li>faithfulness - the factual consistency of the answer to the context base on the question.</li> <li>context_precision - a measure of how relevant the retrieved context is to the question. Conveys quality of the retrieval pipeline.</li> <li>answer_relevancy - a measure of how relevant the answer is to the question</li> <li>context_recall: measures the ability of the retriever to retrieve all the necessary information needed to answer the question.</li> <li>Harmfulness: reducing harmful outputs.</li> <li>PII: making sure senstive user data is not leaked by mistake.</li> </ul>","tags":["llm","development","evaluation","best-practices"]},{"location":"blog/2024/01/01/evaluation-driven-development/#how-does-this-affect-your-product-kpis","title":"How does this affect your product KPIs?","text":"<p>Evals will make sure your NPS from users increases, while making the need to log each interaction less important, since there's an established metric for output quality.</p> <p>Think of using evals as having a QA engineer in form of an SDK!</p>","tags":["llm","development","evaluation","best-practices"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/","title":"2025 03 03 llm evaluations","text":"","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#introduction-to-llm-evaluations","title":"Introduction to LLM Evaluations","text":"<p>Large Language Model (LLM) evaluations are the processes and metrics used to measure how well an LLM performs on a given task or meets certain quality criteria. Evaluating an LLM means defining what \"good\" output looks like\u2014whether that's accuracy, relevance, or safety\u2014and then checking the model's outputs against those expectations. Robust evaluation is critical because it's the only way to know if your model is working as intended and to continuously improve it. In my experience, many AI products that have failed share one common root cause: they never built a reliable evaluation system. Conversely, teams that evaluate early and often can iterate faster and catch problems before users do.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#the-virtuous-cycle-of-llm-development","title":"The Virtuous Cycle of LLM Development","text":"<p>At the heart of LLM development is a virtuous cycle where continuous evaluation and curation enable fast iteration. By testing outputs, identifying weaknesses, and rapidly iterating on improvements, you turn a decent prototype into a trustworthy, high-performing system.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#why-llm-evaluations-matter","title":"Why LLM Evaluations Matter","text":"<p>LLMs can behave unpredictably or degrade as you tweak prompts or scale usage. Relying on initial \"vibe-checks\" is often misleading. Systematic evaluations provide ground truth signals about quality and ensure you're not just shipping a cool demo, but a reliable product. Evaluation results not only help in debugging but also guide improvements, much like software tests catch bugs. In short, a solid evaluation process creates a feedback loop: test, identify flaws, fix, and test again.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#quantitative-vs-qualitative-evaluations","title":"Quantitative vs. Qualitative Evaluations","text":"<p>Broadly speaking, there are two main approaches to evaluating LLMs:</p> <ul> <li> <p>Quantitative Evaluations: These are automated and numeric. For example, for a classification task you might compute accuracy or F1 scores; for translation, metrics like BLEU; and for summarization, metrics like ROUGE. More recently, some teams have started using LLM-based evaluators that assign numeric scores or make pairwise comparisons. Quantitative methods are fast and scalable, but they work best only when the metric truly correlates with real quality.</p> </li> <li> <p>Qualitative Evaluations: These rely on human judgment. Domain experts or end users assess the outputs for aspects such as correctness, clarity, and overall usefulness. While human evaluations are considered the gold standard for subjective tasks, they are slow, expensive, and do not scale as easily.</p> </li> </ul> <p>In practice, a combination of both methods works best. You might use automated tests to filter out the obvious issues and then apply human review on a smaller subset for a more nuanced assessment.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#using-llm-evaluations-in-production","title":"Using LLM Evaluations in Production","text":"<p>Evaluating LLMs isn't a one-time research exercise\u2014it's an ongoing process, especially when models are deployed in production. Setting up an evaluation pipeline early saves you from nasty surprises after deployment. Here's a common layered approach:</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#level-1-unit-tests","title":"Level 1: Unit Tests","text":"<p>Unit tests are assertion-based checks on model outputs, much like traditional software unit tests. You define test inputs and expected outputs or properties, and then automatically verify that the model meets those expectations. For example, if you're building a chatbot, you might assert that when a user asks for pricing info, the response contains a dollar amount. These tests run quickly and cheaply and can catch regressions immediately.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#pseudo-code-example","title":"Pseudo-code Example:","text":"<pre><code># Pseudo-code: simple unit test for a summarization prompt\ninput_text = \"Long article about climate science...\"\nsummary = llm.summarize(input_text)\n# Expect the summary to mention key entities from the article\nassert \"climate\" in summary and \"carbon\" in summary, \"Missing key info in summary\"\n</code></pre>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#level-2-model-driven-and-human-evaluations","title":"Level 2: Model-Driven and Human Evaluations","text":"<p>This level involves deeper evaluation, conducted periodically (for example, nightly or with each model update). It may include logging model outputs and having either humans or an LLM-based judge score them. Comparing these scores helps determine the reliability of automated evaluations. While more insightful, these evaluations require additional time and resources.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#level-3-ab-tests-and-user-metrics","title":"Level 3: A/B Tests and User Metrics","text":"<p>Ultimately, the best judge is your end-user. In production, monitoring how model changes impact user behavior and key performance metrics is essential. A/B tests\u2014deploying a new model version to a fraction of users\u2014can reveal differences in engagement, task success, error rates, and overall satisfaction. Although this approach is the gold standard for measuring business impact, it is also the most resource-intensive and requires thorough vetting with Level 1 and Level 2 evaluations beforehand.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#handling-real-world-constraints","title":"Handling Real-World Constraints","text":"<p>When deploying LLMs, it's important to balance evaluation rigor with practical constraints:</p> <ul> <li>Latency: For applications that need real-time responses (such as live chatbots), inline evaluations must be lightweight. Complex checks might be better suited for offline analysis or asynchronous processing.</li> <li>Cost: Running evaluations, especially with large models, can become expensive. Mitigate this by sampling a subset of traffic for deep evaluation or by caching results.</li> <li>Feedback Loops: Production systems generate a continuous stream of real user data. Logging inputs, outputs, and user interactions\u2014such as clicks or retries\u2014provides invaluable data that can be fed back into the evaluation pipeline for ongoing improvement.</li> </ul>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#pitfalls-and-challenges-in-llm-evaluations","title":"Pitfalls and Challenges in LLM Evaluations","text":"<p>Evaluating LLMs is a nuanced art, and there are several common pitfalls to avoid:</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#metric-obsession-without-purpose","title":"Metric Obsession Without Purpose","text":"<p>Collecting a plethora of metrics can result in a data overload that provides little actionable insight. It's important to focus on a few key criteria that directly tie to user needs or system goals. Vague or uncalibrated scores often end up being \"nice numbers\" that are hard to interpret or act upon.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#ignoring-domain-expertise","title":"Ignoring Domain Expertise","text":"<p>Designing evaluation criteria without input from domain experts can lead to missing what really matters for your specific application. Whether you're dealing with legal documents, medical advice, or any other specialized field, the evaluation should reflect what actual users care about.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#no-systematic-evaluation","title":"No Systematic Evaluation","text":"<p>Relying solely on ad-hoc \"vibe checks\" can leave your system vulnerable to unexpected failures. Establishing a systematic evaluation suite\u2014even if it's just a dozen representative test cases\u2014is essential before shipping a product.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#overfitting-to-benchmarks","title":"Overfitting to Benchmarks","text":"<p>Focusing too narrowly on a single benchmark can lead to models that excel in that narrow area but fail in real-world applications. The goal should be balanced performance across all relevant aspects rather than chasing a single metric.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#trusting-automated-judges-blindly","title":"Trusting Automated Judges Blindly","text":"<p>Automated evaluators, including LLM-based judges, can have their own biases. It's crucial to regularly calibrate these evaluations against human feedback to ensure that the scores reflect true quality.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#lack-of-statistical-rigor","title":"Lack of Statistical Rigor","text":"<p>LLM outputs can be highly variable. Testing on a small sample may lead to conclusions that are simply the result of chance. Always ensure that your experiments are statistically significant, using larger sample sizes and appropriate statistical tests.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#rule-based-workflows-for-llm-evaluations","title":"Rule-Based Workflows for LLM Evaluations","text":"<p>One powerful approach to LLM evaluation is the use of rule-based workflows. Instead of relying solely on learned metrics or subjective human judgment, rule-based evaluations use explicit rules or tests to verify that outputs meet specific criteria. For example, if an LLM generates SQL queries, you can actually execute the query to see if it runs without errors. For summarization, you might enforce that all proper nouns from the source appear in the summary.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#why-rule-based-evaluations","title":"Why Rule-Based Evaluations?","text":"<ul> <li>Speed and Determinism: They're fast, deterministic, and highly interpretable.</li> <li>Cost-Effectiveness: Simple string or structural checks can run in milliseconds, making them suitable for real-time applications.</li> <li>Clear Guardrails: They ensure that critical requirements are met, while more nuanced qualities can be assessed with model-based evaluations or human review.</li> </ul>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#hybrid-evaluation-workflows","title":"Hybrid Evaluation Workflows","text":"<p>The best evaluation systems combine multiple methods to capture both objective criteria and subjective quality. A typical hybrid workflow might include:</p> <ol> <li>Rule-Based Checks: Filter out outputs that fail obvious requirements (e.g., format, required keywords, disallowed content).</li> <li>LLM-Based or Automated Scoring: Assess more subjective qualities such as coherence, relevance, or helpfulness.</li> <li>Human Review: Provide a final layer of quality control by reviewing a sample of outputs, especially those flagged by automated systems.</li> </ol> <p>For instance, in a chatbot application, rules can enforce politeness and required phrases, an LLM evaluator can score the overall response quality, and human reviewers can examine borderline cases to ensure the system truly aligns with user needs.</p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#case-studies-and-industry-learnings","title":"Case Studies and Industry Learnings","text":"<p>Over the years, I've learned several valuable lessons from both successes and failures in LLM evaluations:</p> <ul> <li>Early and Frequent Evaluation: Building a domain-specific evaluation system from the start is crucial. A layered approach\u2014with unit tests, periodic deep evaluations, and A/B testing\u2014enables rapid iteration and early detection of issues.</li> <li>Continuous Feedback Loops: Leveraging real-world data to continuously refine evaluation criteria is essential. A system that constantly logs and learns from each interaction can drive ongoing improvements.</li> <li>Balanced Metrics: Avoid over-optimizing for a single benchmark. Ensure that improvements in one area do not lead to regressions in others.</li> <li>Calibrated Automation: Regularly compare automated evaluation results with human judgment. This ensures that your evaluation methods remain aligned with what users actually value.</li> </ul>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"blog/2025/03/03/2025-03-03-llm-evaluations/#conclusion","title":"Conclusion","text":"<p>LLM evaluations are not just an academic exercise\u2014they are a fundamental component of building reliable, user-friendly AI systems. By defining clear success criteria, using a blend of quantitative and qualitative methods, and integrating evaluations into your production workflow, you can catch issues early and continuously improve your models. Though evaluations may not be the most glamorous part of AI development, they are the unsung hero that transforms an impressive demo into a trustworthy, production-ready product.</p> <p>Investing in a robust evaluation system unlocks superpowers for any AI team, enabling rapid fine-tuning, effective debugging, and, ultimately, the delivery of AI products that users can trust. </p>","tags":["LLM Evaluation","AI Development","Machine Learning","Production AI"]},{"location":"rag/","title":"RAG Systems","text":"<p>Retrieval Augmented Generation (RAG) is a powerful approach that combines the capabilities of large language models with external knowledge retrieval. This section covers best practices, case studies, and implementation details for building effective RAG systems.</p>"},{"location":"rag/#topics-covered","title":"Topics Covered","text":"<ul> <li>Best Practices - Guidelines and recommendations for building robust RAG systems</li> <li>Case Studies - Real-world examples and implementations</li> <li>Technical deep dives into specific RAG components</li> </ul>"},{"location":"rag/#getting-started","title":"Getting Started","text":"<p>If you're new to RAG, start with the Best Practices guide to understand the fundamental concepts and common pitfalls to avoid. </p>"},{"location":"rag/best-practices/","title":"RAG Best Practices","text":"<p>This guide covers essential best practices for building effective RAG systems.</p>"},{"location":"rag/best-practices/#1-document-processing","title":"1. Document Processing","text":"<ul> <li>Clean and preprocess documents thoroughly</li> <li>Implement effective chunking strategies</li> <li>Maintain document metadata</li> <li>Handle different document formats consistently</li> </ul>"},{"location":"rag/best-practices/#2-vector-storage","title":"2. Vector Storage","text":"<ul> <li>Choose appropriate embedding models</li> <li>Optimize vector dimensions</li> <li>Implement efficient indexing</li> <li>Consider hybrid search approaches</li> </ul>"},{"location":"rag/best-practices/#3-retrieval-strategy","title":"3. Retrieval Strategy","text":"<ul> <li>Design effective query processing</li> <li>Implement re-ranking mechanisms</li> <li>Use semantic and keyword search</li> <li>Handle edge cases and failures</li> </ul>"},{"location":"rag/best-practices/#4-response-generation","title":"4. Response Generation","text":"<ul> <li>Implement proper prompt engineering</li> <li>Handle context limitations</li> <li>Ensure source attribution</li> <li>Validate generated responses</li> </ul>"},{"location":"rag/best-practices/#5-system-monitoring","title":"5. System Monitoring","text":"<ul> <li>Track retrieval quality metrics</li> <li>Monitor embedding consistency</li> <li>Implement feedback loops</li> <li>Log system performance </li> </ul>"},{"location":"rag/case-studies/","title":"RAG Case Studies","text":"<p>Real-world examples of RAG system implementations and their outcomes.</p>"},{"location":"rag/case-studies/#enterprise-knowledge-base","title":"Enterprise Knowledge Base","text":""},{"location":"rag/case-studies/#challenge","title":"Challenge","text":"<ul> <li>Large volume of internal documentation</li> <li>Multiple document formats</li> <li>Need for real-time access</li> <li>Security requirements</li> </ul>"},{"location":"rag/case-studies/#solution","title":"Solution","text":"<ul> <li>Implemented hybrid chunking strategy</li> <li>Used domain-specific embeddings</li> <li>Built custom reranking pipeline</li> <li>Integrated with SSO</li> </ul>"},{"location":"rag/case-studies/#results","title":"Results","text":"<ul> <li>80% reduction in search time</li> <li>90% accuracy in retrievals</li> <li>Improved employee productivity</li> </ul>"},{"location":"rag/case-studies/#customer-support-system","title":"Customer Support System","text":""},{"location":"rag/case-studies/#challenge_1","title":"Challenge","text":"<ul> <li>High volume of support tickets</li> <li>Need for consistent responses</li> <li>Multiple product lines</li> <li>Multiple languages</li> </ul>"},{"location":"rag/case-studies/#solution_1","title":"Solution","text":"<ul> <li>Implemented multilingual embeddings</li> <li>Built product-specific indexes</li> <li>Created response templates</li> <li>Integrated feedback loop</li> </ul>"},{"location":"rag/case-studies/#results_1","title":"Results","text":"<ul> <li>60% faster response time</li> <li>40% reduction in escalations</li> <li>Improved customer satisfaction</li> </ul>"},{"location":"rag/case-studies/#technical-documentation","title":"Technical Documentation","text":""},{"location":"rag/case-studies/#challenge_2","title":"Challenge","text":"<ul> <li>Complex technical content</li> <li>Frequent updates</li> <li>Version control requirements</li> <li>Code snippets and diagrams</li> </ul>"},{"location":"rag/case-studies/#solution_2","title":"Solution","text":"<ul> <li>Implemented version-aware indexing</li> <li>Built code-specific chunking</li> <li>Created diagram extraction pipeline</li> <li>Integrated with Git workflow</li> </ul>"},{"location":"rag/case-studies/#results_2","title":"Results","text":"<ul> <li>70% faster documentation searches</li> <li>85% accuracy in code snippet retrieval</li> <li>Improved developer experience </li> </ul>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/archive/2022/","title":"2022","text":""},{"location":"blog/category/llm/","title":"LLM","text":""},{"location":"blog/category/evaluation/","title":"Evaluation","text":""},{"location":"blog/category/ai/","title":"AI","text":""}]}